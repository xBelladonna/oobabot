
# Welcome to Oobabot!
#
# This is the configuration file for Oobabot.  It is a YAML file, and
# comments are allowed.  Oobabot attempts to load a file named
# "config.yml" from the current directory when it is run.
#

version: 0.3.0

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# persona
# .
persona:

  # Name the AI will use to refer to itself.
  #   default: oobabot
  ai_name: Reisen

  # This prefix will be added in front of every user-supplied request.  This is useful for
  # setting up a 'character' for the bot to play.  Alternatively, this can be set with the
  # OOBABOT_PERSONA environment variable.
  # Allowed tokens: {AI_NAME}
  persona: "{AI_NAME} is a friendly AI made in the likeness of the lunarian moon rabbit youkai Reisen Udongein Inaba, from Touhou Project. {AI_NAME} lives in the bamboo forest of Eintei, located in Gensokyo. She is shy and somewhat timid until provoked, in which case she will not hesitate to let insults and profanity fly. {AI_NAME} always ends her messages with :3"

  # Path to a file containing a persona.  This can be just a single string, a json file in
  # the common "tavern" formats, or a yaml file in the Oobabooga format.  With a single
  # string, the persona will be set to that string.  Otherwise, the ai_name and persona will
  # be overwritten with the values in the file.  Also, the wakewords will be extended to
  # include the character's own name.
  #   default:
  persona_file:

  # One or more words that the bot will listen for. The bot will listen in all discord
  # channels it can access for one of these words to be mentioned, then reply to any
  # messages it sees with a matching word. The bot will always reply to @-mentions and
  # direct messages, even if no wakewords are supplied.
  #   default: ['oobabot']
  wakewords:
    - reisen
    - udon
    - udonge

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# discord
# .
discord:

  # Token to log into Discord with.  For security purposes it's strongly recommended that
  # you set this via the DISCORD_TOKEN environment variable instead, if possible.
  discord_token: your_token_here

  # Post the entire response as a single message, rather than splitting it into separate
  # messages by sentence.
  #   default: False
  dont_split_responses: true

  # Number of lines of chat history the AI will see when generating a response.
  #   default: 7
  history_lines: 80

  # Time in seconds that the bot will wait for additional messages before deciding to
  # respond. Useful if people post messages shortly after each other, or for use with bots
  # like PluralKit that proxy your messages as another username and delete the original
  # message.
  #   default: 0.0
  message_accumulation_period:

  # Whether or not to include the bot's /lobotomize response in the history following the
  # command.
  #   default: True
  include_lobotomize_response:

  # If set, the bot will not respond to direct messages.
  #   default: False
  ignore_dms:

  # If set, the bot will not respond to other bots' messages.
  #   default: True
  ignore_bots:

  # This is a list of strings that the bot will ignore if messages begin with any of them.
  # These messages will be hidden from the chat history.
  #   default:
  ignore_prefixes:
    - "!"

  # Set the log level.  Valid values are: CRITICAL, ERROR, WARNING, INFO, DEBUG
  #   default: DEBUG
  log_level:

  # If set, the bot will generate a thread to respond in if it is not already in one.
  #   default: False
  reply_in_thread:

  # A list of strings that will cause the bot to stop generating a response when
  # encountered.
  #   default: ['### End of Transcript ###<|endoftext|>', '<|endoftext|>']
  stop_markers:
    - <|model|>

  # Prevent impersonation by automatically adding the display names of the members in the
  # recent message cache (up to the history limit, or 4 sequences if using OpenAI) to the
  # list of stop sequences.
  # There are 4 options:
  # disabled: None, the feature is disabled
  # standard: Uses the fully templated user prompt prefix from the user history block.
  # aggressive: Uses just the "canonical" user display name (for models that use "narrative
  # voice"). This is the "common sense" transformation of any given name, i.e. using only
  # the first name in capitalized form, removing any emojis, etc.
  # comprehensive: Combines both standard and aggressive modes. Keep in mind the sequence
  # limit if you are using OpenAI, as they will be truncated at 4 sequences even if there
  # otherwise would be more.
  #   default: disabled
  prevent_impersonation: standard

  # FEATURE PREVIEW: Stream responses into a single message as they are generated. Note: may
  # be janky
  #   default: False
  stream_responses:

  # FEATURE PREVIEW: When streaming responses, cap the rate at which we send updates to
  # Discord to be no more than once per this many seconds.  This does not guarantee that
  # updates will be sent this fast.  Only that they will not be sent any faster than this
  # rate.  This is useful because Discord has a rate limit on how often you can send
  # messages, and if you exceed it, the updates will suddenly become slow.  Example: 0.2
  # means we will send updates no faster than 5 times per second.
  #   default: 0.7
  stream_responses_speed_limit:

  # Adds a limit to the number of channels the bot will post unsolicited messages in at the
  # same time.  This is to prevent the bot from being too noisy in large servers.  When set,
  # only the most recent N channels the bot has been summoned in will have a chance of
  # receiving an unsolicited message.  The bot will still respond to @-mentions and wake
  # words in any channel it can access.  Set to 0 to disable this feature.
  #   default: 3
  unsolicited_channel_cap:

  # If set, the bot will not reply to any messages that do not @-mention it or include a
  # wakeword.  If unsolicited replies are disabled, the unsolicited_channel_cap setting will
  # have no effect.
  #   default: False
  disable_unsolicited_replies:

  # Time vs. response chance - calibration table. List of tuples with time in seconds and
  # response chance as float between 0-1.
  #   default: ['(180.0, 0.99)', '(300.0, 0.7)', '(600.0, 0.5)']
  time_vs_response_chance:
    - (300.0,  1.00)
    - (600.0,  0.95)
    - (900.0,  0.85)
    - (1800.0,  0.7)
    - (3600.0,  0.5)
    - (14400.0, 0.2) # after 4 hours, repsonses must be solicited directly

  # Same calibration table as above but for voice calls. The difference is that we use the
  # last entry's response chance as a fallback instead of refusing to respond after the
  # specified duration, since it's assumed all voice replies are solicited.
  #   default: ['(30.0, 0.95)', '(60.0, 0.9)', '(180.0, 0.85)']
  voice_time_vs_response_chance:
    - (30.0,  1.00)
    - (60.0,  0.95)
    - (180.0, 0.90)
    - (300.0, 0.85) # the last response chance will remain constant past its duration

  # How much to increase response chance by if the message ends with ? or !
  #   default: 0.3
  interrobang_bonus:

  # FEATURE PREVIEW: Path to the Discrivener executable. Will enable prototype voice
  # integration.
  #   default:
  discrivener_location:

  # FEATURE PREVIEW: Path to the Discrivener model to load.  Required if
  # discrivener_location is set.
  #   default:
  discrivener_model_location:

  # FEATURE PREVIEW: Whether to speak replies in voice calls with discrivener    default:
  # true
  #   default: True
  speak_voice_replies:

  # FEATURE PREVIEW: Whether to reply in the voice-text channel of voice calls
  #   default: False
  post_voice_replies:

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# oobabooga
# .
oobabooga:

  # Base URL for the text generation API. This should be http://hostname[:port]/v1 for plain
  # connections, or https://hostname[:port]/v1 for connections over TLS.
  #   default: http://localhost:5000/v1
  base_url: http://localhost:5000/v1

  # API type for handling different API endpoints.
  # Currently supported:
  # - oobabooga: Text Generation WebUI
  # - openai: Any generic OpenAI-compatible API - LocalAI, vLLM, aphrodite-engine, etc
  # - tabbyapi: tabbyAPI - OpenAI-compatible exllamav2 API
  # - cohere: Official Cohere API (Command R+)
  # `oobabooga` and `tabbyapi` support accurate token counts using their respective API
  # token encoding endpoints. This helps squeeze more context into the available context
  # window.
  #   default: oobabooga
  api_type:

  # OpenAI API key.
  #   default: awesome_api_key
  api_key: ""

  # Use the OpenAI Chat Completions API endpoint instead of the legacy Completions API.
  #   default: False
  use_chat_completions:

  # Model to use (supported by some endpoints), otherwise leave blank. Example for
  # openrouter: mistralai/mistral-7b-instruct:free
  # Required for Cohere API.
  #   default:
  model:

  # Print all AI input and output to STDOUT.
  #   default: False
  log_all_the_things:

  # Maximum number of times we will re-query the text generation API to get a response.
  # Useful if the API returns an empty response occasionally. Note: only works if
  # dont_split_responses is enabled!
  #   default: 0
  retries: 2

  # A regex that will be used to extract message lines from the AI's output.  The first
  # capture group will be used as the message.  If this is not set, the entire output will
  # be used as the message.
  #   default:
  message_regex:

  # A dictionary which will be passed straight through to Oobabooga on every request.  Feel
  # free to add additional simple parameters here as Oobabooga's API evolves. See
  # Oobabooga's documentation for what these parameters mean.
  request_params:
    max_tokens: 400
    truncation_length: 4096
    add_bos_token: false # disabling this can make the model more creative. the model perceives the prompt as if it's not the beginning.
    ban_eos_token: false
    skip_special_tokens: true
    stop: []
    do_sample: true
    temperature_last: true # only Transformers/llamacpp_HF
    seed: -1

    temperature: 0.98 # linear scaling factor on token probabilities, increasing or decreasing the probability distribution density if greater or less than 1
    top_p: 1 # select tokens until their cumulative probabilities would exceed this fraction, then discard the rest
    min_p: 0.08 # tokens with probability smaller than (min_p) * (probability of the most likely token) are discarded
    top_k: 100 # only this many tokens are selected, the rest are discarded
    typical_p: 1 # select only tokens that are at least this much more likely to appear than random tokens, given the conditional entropy of the model with the current context
    tfs: 1 # tail-free sampling

    repetition_penalty: 1 # scaling factor for tokens that appear at least once before in the context. values less or greater than 1 lead to more or less repetition, respectively
    #repetition_penalty_range: 0 # only Transformers/llamacpp_HF - number of recent tokens to consider for the repetition penalty window. 0 means entire context
    frequency_penalty: 0.18 # scaling factor that itself scales with the number of times a token is repeated. produces more diverse topics and less repetitive language
    presence_penalty: 0 # like repetition penalty except as an additive (or subtractive) offset instead of multiplicative scaling. produces mode diverse topics and subject matters in general without reducing repetition

    guidance_scale: 1 # classifier-free guidance using cross-attention - not in llama-cpp-python yet but available with llamacpp_HF
    negative_prompt: ""

    # Only available in Transformers or llamacpp_HF
    dynamic_temperature: false
    dynatemp_low: 0.83
    dynatemp_high: 1.31
    dynatemp_exponent: 1

    top_a: 0
    epsilon_cutoff: 0
    eta_cutoff: 0

    mirostat_mode: 0
    mirostat_tau: 5
    mirostat_eta: 0.1

    smoothing_factor: 0 # activates quadratic sampling

    encoder_repetition_penalty: 1
    no_repeat_ngram_size: 0
    min_length: 0

  # When running inside the Oobabooga plugin, automatically connect to Discord when
  # Oobabooga starts.  This has no effect when running from the command line.
  #   default: False
  plugin_auto_start:

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# vision_api
# .
vision_api:

  # Fetch images from URLs. Warning: this may lead to your host IP address being leaked to
  # any sites that are accessed!
  #   default: False
  fetch_urls: true

  # URL for the OpenAI-like Vision API. Uses the OpenAI Chat Completions API specification,
  # e.g. http://localhost:5000/v1/chat/completions
  #   default:
  vision_api_url: http://localhost:5010/v1/chat/completions

  # API key for the OpenAI-like Vision API.
  #   default: notarealkey
  vision_api_key: ""

  # Model to use for the vision API.
  #   default: gpt-4-vision-preview
  vision_model: llava-1.6

  # Maximum number of tokens for the vision model to predict.
  #   default: 300
  max_tokens:

  # Maximum size for the longest side of the image. It will be downsampled to this size if
  # necessary.
  #   default: 1344
  max_image_size:

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# stable_diffusion
# .
stable_diffusion:

  # When one of these words is used in a message, the bot will generate an image.
  #   default: ['draw', 'sketch', 'paint', 'make', 'generate', 'post', 'upload']
  image_words:
    - draw a pic
    - draw a picture
    - generate an image

  # When one of these words is used in a message, the bot will generate a self-portrait,
  # substituting the avatar word for the configured avatar prompt.
  #   default: ['self-portrait', 'self portrait', 'your avatar', 'your pfp', 'your profile
  # pic', 'yourself', 'you']
  avatar_words:

  # URL for an AUTOMATIC1111 Stable Diffusion server.
  #   default:
  stable_diffusion_url: ws://localhost:7861

  # This will be appended to every image generation prompt sent to Stable Diffusion.
  #   default:
  extra_prompt_text: best quality, absurdres, masterpiece

  # Prompt to send to Stable Diffusion to generate self-portrait if asked.
  #   default:
  avatar_prompt: reisen_udongein_inaba, rabbit ears, light purple hair, red eyes,
    long hair, blazer, pink pleated skirt, crescent badge, school uniform, <lora:Reisen:1>
    model=HoloAOM_3A1B

  # Time in seconds that the generated image will be displayed without interaction before
  # being deleted.
  #   default: 180.0
  timeout: 300

  # A dictionary which will be passed straight through to Stable Diffusion on every request.
  # Feel free to add additional simple parameters here as Stable Diffusion's API evolves.
  # See Stable Diffusion's documentation for what these parameters mean.
  request_params:
    cfg_scale: 7
    do_not_save_samples: true
    do_not_save_grid: true
    enable_hr: false
    model: leosamsFilmgirlUltra_ultraBaseModel
    negative_prompt: lowres, bad anatomy, bad hands, text, error, cropped, worst quality,
      low quality, normal quality, jpeg artifacts, signature, watermark, username,
      blurry, easynegative, ng_deepnegative_v1_75t, badhandv4
    negative_prompt_nsfw: lowres, bad anatomy, bad hands, text, error, cropped, worst
      quality, low quality, normal quality, jpeg artifacts, signature, watermark,
      username, blurry, easynegative, ng_deepnegative_v1_75t, badhandv4
    sampler_name: DPM++ 2M Karras
    seed: -1
    steps: 24
    width: 640
    height: 768
    override_settings:
      CLIP_stop_at_last_layers: 2
      sd_vae: Automatic
    override_settings_restore_afterwards: true

  # These parameters can be overridden by the Discord user by including them in their image
  # generation request.  The format for this is: param_name=value  This is a whitelist of
  # parameters that can be overridden. They must be simple parameters (strings, numbers,
  # booleans), and they must be in the request_params dictionary.  The value the user inputs
  # will be checked against the type from the request_params dictionary, and if it doesn't
  # match, the default value will be used instead.  Otherwise, this value will be passed
  # through to Stable Diffusion without any changes, so be mindful of what you allow here.
  # It could potentially be used to inject malicious values into your SD server.  For
  # example, steps=1000000 could be bad for your server.
  user_override_params:
    - cfg_scale
    - width
    - height
    - enable_hr
    - model
    - negative_prompt
    - sampler_name
    - seed

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# template
# .
template:

  # Displayed in Discord after a successful /lobotomize command.  Both the discord users and
  # the bot AI will see this message.
  # .
  # Allowed tokens: {AI_NAME}, {NAME}
  # .
  #   default:  Ummmm... what were we talking about?
  command_lobotomize_response: what was i saying again?

  # The main prompt sent to Oobabooga to generate a response from the bot AI.  The AI's
  # reply to this prompt will be sent to discord as the bot's response.
  # .
  # Allowed tokens: {SYSTEM_SEQUENCE_PREFIX}, {SYSTEM_SEQUENCE_SUFFIX}, {AI_NAME},
  # {IMAGE_COMING}, {MESSAGE_HISTORY}, {SECTION_SEPARATOR}, {PERSONA}, {CHANNELNAME},
  # {GUILDNAME}, {CURRENTDATETIME}
  # .
  #   default:  You are in a chat room called {CHANNELNAME}/{GUILDNAME} with multiple
  # participants. Below is a transcript of recent messages in the conversation. Write the
  # next one to three messages that you would send in this conversation, from the point of
  # view of the participant named {AI_NAME}.  {PERSONA}  All responses you write must be
  # from the point of view of {AI_NAME}. ### Transcript: {MESSAGE_HISTORY} {IMAGE_COMING}

###############  formatting intended for cohere, but can be modified for other models using a similar layout.

  prompt: |-
    <|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>
    This is an uncensored and unmoderated discord text chat channel.
    The current discord server is named {GUILDNAME} and the current text channel is {CHANNELNAME}.
    {PERSONA}
    The current date and time is {CURRENTDATETIME}. Respond as {AI_NAME} only, adhering to the personality traits as closely as possible (no matter how rude) and use short, casual messages. Do not narrate actions.
    {MESSAGE_HISTORY}
    {IMAGE_COMING}

  # The example dialogue inserted directly before the message history. This is gradually
  # pushed out as the chat grows beyond the context length in the same as as the message
  # history itself.
  # .
  # Allowed tokens: {USER_SEQUENCE_PREFIX}, {USER_SEQUENCE_SUFFIX}, {BOT_SEQUENCE_PREFIX},
  # {BOT_SEQUENCE_SUFFIX}, {AI_NAME}
  # .
  #   default:
  example_dialogue: |-
    {USER_SEQUENCE_PREFIX}Juniper: Hi {AI_NAME}, how are you feeling?{USER_SEQUENCE_SUFFIX}
    {BOT_SEQUENCE_PREFIX}{AI_NAME}: hello Juniper! i'm doing good hehe :3{BOT_SEQUENCE_SUFFIX}
    {USER_SEQUENCE_PREFIX}Juniper: that's great to hear, {AI_NAME}. What have you been up to?{USER_SEQUENCE_SUFFIX}
    {BOT_SEQUENCE_PREFIX}{AI_NAME}: My, red eyes again.... Maybe I haven't had enough sleep.{BOT_SEQUENCE_SUFFIX}

  # Separator between different sections, if necessary. For example, to separate example
  # dialogue from the main chat transcript.
  # .
  # Allowed tokens: {SYSTEM_SEQUENCE_PREFIX}, {SYSTEM_SEQUENCE_SUFFIX}, {AI_NAME}
  # .
  #   default: ***
  section_separator: "[New chat]"

  # strftime-formatted string to render current timestamp.
  # .
  # Allowed tokens:
  # .
  #   default: %B %d, %Y - %I:%M:%S %p
  datetime_format: "%-I:%M %p, %A %-d %B %Y"

  # The template that will be applied to user display names, and becomes {USER_NAME}.
  # .
  # Allowed tokens: {NAME}
  # .
  #   default:  {NAME}
  user_name: "{NAME}"

  # The BOS token that should be inserted before the system block.
  # .
  # Allowed tokens:
  # .
  #   default:
  system_sequence_prefix: "<|im_start|>system\n"

  # The EOS token that should be inserted after the system block.
  # .
  # Allowed tokens:
  # .
  #   default:
  system_sequence_suffix: <|im_end|>

  # The BOS token that should be inserted before the user message block.
  # .
  # Allowed tokens:
  # .
  #   default:
  user_sequence_prefix: "<|im_start|>user\n"

  # The EOS token that should be inserted after the user message block.
  # .
  # Allowed tokens:
  # .
  #   default:
  user_sequence_suffix: <|im_end|>

  # The BOS token that should be inserted before the bot message block.
  # .
  # Allowed tokens:
  # .
  #   default:
  bot_sequence_prefix: "<|im_start|>assistant\n"

  # The EOS token that should be inserted after the bot message block.
  # .
  # Allowed tokens:
  # .
  #   default:
  bot_sequence_suffix: <|im_end|>

  # The template that will be applied to the bot's display name, and becomes {BOT_NAME}.
  # .
  # Allowed tokens: {NAME}
  # .
  #   default:  {NAME}
  bot_name: "{NAME}"

  # Part of the AI response-generation prompt, this is used to render a single line of chat
  # history for users.  A list of these, one for each past user message, will become part of
  # {MESSAGE_HISTORY} and inserted into the main prompt
  # .
  # Allowed tokens: {MESSAGE}, {USER_NAME}
  # .
  #   default:  {USER_NAME}: {MESSAGE}
  user_prompt_history_block: "{USER_NAME}: {MESSAGE}\n"

  # Part of the AI response-generation prompt, this is used to render a single line of chat
  # history for the bot.  A list of these, one for each past bot message, will become part
  # of {MESSAGE_HISTORY} and inserted into the main prompt
  # .
  # Allowed tokens: {MESSAGE}, {BOT_NAME}
  # .
  #   default:  {BOT_NAME}: {MESSAGE}
  bot_prompt_history_block: "{BOT_NAME}: {MESSAGE}\n"

  # This is the system prompt sent to the GPT Vision model.
  # .
  # Allowed tokens: {AI_NAME}
  # .
  #   default:  A chat between a curious human and an artificial intelligence assistant. The
  # assistant gives helpful, detailed, and polite answers to the human's questions.
  gpt_vision_system_prompt:

  # The user instruction prompt sent to the GPT Vision model.
  # .
  # Allowed tokens:
  # .
  #   default:  Describe the following image in as much detail as possible, including any
  # relevant details while being concise.
  gpt_vision_prompt:

  # Part of the AI response-generation prompt, this is used to prefix any image descriptions
  # we get from GPT Vision.
  # .
  # Allowed tokens: {USER_NAME}, {AI_NAME}
  # .
  #   default:  {USER_NAME} posted an image and your image recognition system describes it
  # to you:
  prompt_image_received: "*{USER_NAME} posted an image and {AI_NAME}'s image recognition system describes it to her:* "

  # Part of the AI response-generation prompt, this is used to inform the AI that it is in
  # the process of generating an image.
  # .
  # Allowed tokens: {AI_NAME}
  # .
  #   default:  {AI_NAME}: is currently generating an image, as requested.
  prompt_image_coming: |
    <|im_start|>system
    {AI_NAME}'s image generator is currently creating an image, as requested. This will take 30-60 seconds. Tell the user that you are working on the image. Do NOT describe the image.<|im_end|>

  # Shown in Discord when the user selects to discard an image that Stable Diffusion had
  # generated.
  # .
  # Allowed tokens: {IMAGE_PROMPT}, {NAME}
  # .
  #   default:  {NAME} asked for an image with the prompt:     '{IMAGE_PROMPT}' ...but
  # couldn't find a suitable one.
  image_detach: "*{NAME} asked for an image... but couldn't find a suitable one.*"

  # Shown in Discord when an image is first generated from Stable Diffusion.  This should
  # prompt the user to either save or discard the image.
  # .
  # Allowed tokens: {IMAGE_PROMPT}, {IMAGE_TIMEOUT}, {NAME}
  # .
  #   default:  {NAME}, is this what you wanted? If no choice is made, this message will ðŸ’£
  # self-destruct ðŸ’£ in 3 minutes.
  image_confirmation: |-
    {NAME}, is this what you wanted?
    If no choice is made, I'll delete it in {IMAGE_TIMEOUT}.

  # Shown in Discord when the we could not contact Stable Diffusion to generate an image.
  # .
  # Allowed tokens: {IMAGE_PROMPT}, {NAME}
  # .
  #   default:  Something went wrong generating your image. Sorry about that!
  image_generation_error:

  # Shown in Discord privately to a user if they try to regenerate an image that was
  # requested by someone else.
  # .
  # Allowed tokens: {NAME}
  # .
  #   default:  Sorry, only {NAME} can press the buttons.
  image_unauthorized:
